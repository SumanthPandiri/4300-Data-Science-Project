{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "from sklearn import metrics # for the evaluation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listControl = []\n",
    "listPatient = []\n",
    "\n",
    "#Reads in each of the 8 sheets for the patient and the control datasets\n",
    "for i in range(1,9):\n",
    "    #Imports the patient dataset and drops unneccessary rows/columns\n",
    "    df = pd.read_excel('Data/ALL_PMS_Patient_Biolog_data_NORMALIZED.xlsx', sheet_name='PM-M' + str(i), engine='openpyxl')\n",
    "    df = df[1:]\n",
    "    df.columns = df.iloc[0]\n",
    "    df = df[1:]\n",
    "    df = df.drop('well', axis=1)\n",
    "    dfNames = df['CMS#']\n",
    "    df = df.drop('CMS#', axis=1)\n",
    "\n",
    "    #Imports the control dataset and removed unnecessary rows/columns\n",
    "    df_control = pd.read_excel('Data/ABS_Normalized Control_PMS data.xlsx', sheet_name='PM-M' + str(i) + '_Control', engine='openpyxl')\n",
    "    df_control = df_control[4:]\n",
    "    cols = df_control.columns.tolist()\n",
    "    to_remove = cols[0:59] + cols[109:]\n",
    "    df_control = df_control.loc[:, ~df_control.columns.isin(to_remove)]\n",
    "\n",
    "    #Converts to numpy array and adds the numpy array to a list\n",
    "    arrayControl = df_control.to_numpy()\n",
    "    arrayPatient = df.to_numpy()\n",
    "\n",
    "    #Appends the array to a list\n",
    "    listControl.append(arrayControl)\n",
    "    listPatient.append(arrayPatient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converts the lists to arrays and reshapes the arrays to be 2-dimensional instead of 3-dimensional\n",
    "arrayControl_3D = np.array(listControl)\n",
    "arrayControl = arrayControl_3D.reshape(-1,50)\n",
    "arrayPatient_3D = np.array(listPatient)\n",
    "arrayPatient = arrayPatient_3D.reshape(-1,48)\n",
    "\n",
    "print(arrayControl.shape)\n",
    "print(arrayPatient.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medianDifference = []\n",
    "meanDifference = []\n",
    "\n",
    "arrayControl = np.delete(arrayControl, [412,413,414,415], 0)\n",
    "arrayPatient = np.delete(arrayPatient, [412,413,414,415], 0)\n",
    "\n",
    "#Calculate difference of the control median and patient median per well and appends value to the list\n",
    "for i in range(0,764):\n",
    "    controlMedian = np.median(arrayControl[i])\n",
    "    patientMedian = np.median(arrayPatient[i])\n",
    "    medianDifference.append(controlMedian-patientMedian)\n",
    "\n",
    "    controlMean = np.mean(arrayControl[i])\n",
    "    patientMean = np.mean(arrayPatient[i])\n",
    "    meanDifference.append(controlMean-patientMean)\n",
    "\n",
    "#Converts the list to a NumPy array\n",
    "Mean = np.array(meanDifference)\n",
    "Median = np.array(medianDifference)\n",
    "print(np.median(abs(Mean)))\n",
    "print(np.median(abs(Median)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this case, we will use the first 50 columns as the control and the last 48 columns as the patient\n",
    "x_array = np.append(arrayControl, arrayPatient, 1)\n",
    "\n",
    "#Creates a set of values in the median differences array that are between -0.2 and 0.2 and deletes the values from x_array\n",
    "toDrop = np.where(abs(Median) < 0.5)\n",
    "x_array = np.delete(x_array, toDrop, 0)\n",
    "\n",
    "#transposes the x_array\n",
    "x_array_transpose = np.transpose(x_array)\n",
    "print(x_array_transpose.shape)\n",
    "\n",
    "#Creates a corresponding y array where 0 is control and 1 is patient\n",
    "y_array = np.append(np.zeros(50), np.ones(48)) \n",
    "print(y_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listAccuracyLiblinearScore = []\n",
    "listLogLossLiblinearScore = []\n",
    "listRecallLiblinearScore = []\n",
    "listPrecisionLiblinearScore = []\n",
    "listROCAUCLiblinearScore = []\n",
    "listConfusionMatrices =[]\n",
    "\n",
    "#Runs the logistic regression 100 times and calculates the accuracy score\n",
    "for i in range (0,100):\n",
    "    #Randomly shuffles the x and y data while keeping matching order\n",
    "    temp = list(zip(x_array_transpose, y_array))\n",
    "    random.shuffle(temp)\n",
    "    x_shuffled, y_shuffled = zip(*temp)\n",
    "    x_shuffled = np.array(x_shuffled)\n",
    "    y_shuffled = np.array(y_shuffled)\n",
    "\n",
    "    #Splits data into train and test split\n",
    "    x_train = x_shuffled[0:80]\n",
    "    x_test = x_shuffled[80:99]\n",
    "    y_train = y_shuffled[0:80]\n",
    "    y_test = y_shuffled[80:99]\n",
    "\n",
    "    # Fitting Logistic Regression to the Training set\n",
    "    lr = LogisticRegression(solver=\"liblinear\", max_iter=10000)\n",
    "    lr.fit(x_train, y_train)\n",
    "\n",
    "    # Predicting the Test set results\n",
    "    y_pred = lr.predict(x_test)\n",
    "    y_pred_proba =lr.predict_proba(x_test)\n",
    "    listAccuracyLiblinearScore.append(metrics.accuracy_score(y_test, y_pred))\n",
    "    listLogLossLiblinearScore.append(metrics.log_loss(y_test, y_pred_proba))\n",
    "    listRecallLiblinearScore.append(metrics.recall_score(y_test, y_pred))\n",
    "    listPrecisionLiblinearScore.append(metrics.precision_score(y_test, y_pred))\n",
    "    listROCAUCLiblinearScore.append(metrics.roc_auc_score(y_test, y_pred))\n",
    "    listConfusionMatrices.append(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"Accuracy: \" + str(np.mean(listAccuracyLiblinearScore)))\n",
    "print(\"Log Loss: Mean: \" + str(np.mean(listLogLossLiblinearScore)) + \"  Median: \" + str(np.median(listLogLossLiblinearScore)))\n",
    "print(\"Recall: \" + str(np.mean(listRecallLiblinearScore)))\n",
    "print(\"Precision: \" + str(np.mean(listPrecisionLiblinearScore)))\n",
    "print(\"ROC-AUC: \" + str(np.mean(listROCAUCLiblinearScore)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Accuracy across all logistic regressions\")\n",
    "plt.xlabel(\"Regression #\")\n",
    "plt.ylabel(\"Accuracy Score\")\n",
    "plt.plot(listAccuracyLiblinearScore, 'ro', scalex=True)\n",
    "l = [np.mean(listAccuracyLiblinearScore)] * 100\n",
    "plt.plot(np.arange(0, 100), l, color=\"blue\", label=\"mean\")\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Log Loss across all logistic regressions\")\n",
    "plt.xlabel(\"Regression #\")\n",
    "plt.ylabel(\"Log Loss Score\")\n",
    "plt.plot(listLogLossLiblinearScore, 'ro')\n",
    "plt.ylim(0, np.max(listLogLossLiblinearScore))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Recall Score across all logistic regressions\")\n",
    "plt.xlabel(\"Regression #\")\n",
    "plt.ylabel(\"Recall Score\")\n",
    "plt.plot(listRecallLiblinearScore, 'ro')\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Precision across all logistic regressions\")\n",
    "plt.xlabel(\"Regression #\")\n",
    "plt.ylabel(\"Precision Score\")\n",
    "plt.plot(listPrecisionLiblinearScore, 'ro')\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"ROCAUC across all logistic regressions\")\n",
    "plt.xlabel(\"Regression #\")\n",
    "plt.ylabel(\"ROCAUC Score\")\n",
    "plt.plot(listPrecisionLiblinearScore, 'ro')\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listAccuracySagaScore = []\n",
    "listLogLossSagaScore = []\n",
    "listRecallSagaScore = []\n",
    "listPrecisionSagaScore = []\n",
    "listROCAUCSagaScore = []\n",
    "\n",
    "#Runs the logistic regression 100 times and calculates the accuracy score\n",
    "for i in range (0,100):\n",
    "    #Randomly shuffles the x and y data while keeping matching order\n",
    "    temp = list(zip(x_array_transpose, y_array))\n",
    "    random.shuffle(temp)\n",
    "    x_shuffled, y_shuffled = zip(*temp)\n",
    "    x_shuffled = np.array(x_shuffled)\n",
    "    y_shuffled = np.array(y_shuffled)\n",
    "\n",
    "    #Splits data into train and test split\n",
    "    x_train = x_shuffled[0:80]\n",
    "    x_test = x_shuffled[80:99]\n",
    "    y_train = y_shuffled[0:80]\n",
    "    y_test = y_shuffled[80:99]\n",
    "\n",
    "    # Fitting Logistic Regression to the Training set\n",
    "    lr = LogisticRegression(solver=\"saga\", max_iter=10000)\n",
    "    lr.fit(x_train, y_train)\n",
    "\n",
    "    # Predicting the Test set results\n",
    "    y_pred = lr.predict(x_test)\n",
    "    y_pred_proba =lr.predict_proba(x_test)\n",
    "    listAccuracySagaScore.append(metrics.accuracy_score(y_test, y_pred))\n",
    "    listLogLossSagaScore.append(metrics.log_loss(y_test, y_pred_proba))\n",
    "    listRecallSagaScore.append(metrics.recall_score(y_test, y_pred))\n",
    "    listPrecisionSagaScore.append(metrics.precision_score(y_test, y_pred))\n",
    "    listROCAUCSagaScore.append(metrics.roc_auc_score(y_test, y_pred))\n",
    "\n",
    "print(\"Accuracy: \" + str(np.mean(listAccuracySagaScore)))\n",
    "print(\"Log Loss: Mean: \" + str(np.mean(listLogLossSagaScore)) + \"  Median: \" + str(np.median(listLogLossSagaScore)))\n",
    "print(\"Recall: \" + str(np.mean(listRecallSagaScore)))\n",
    "print(\"Precision: \" + str(np.mean(listPrecisionSagaScore)))\n",
    "print(\"ROC-AUC: \" + str(np.mean(listROCAUCSagaScore)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Accuracy across all logistic regressions\")\n",
    "plt.xlabel(\"Regression #\")\n",
    "plt.ylabel(\"Accuracy Score\")\n",
    "plt.plot(listAccuracySagaScore, 'ko', label=\"Saga\")\n",
    "plt.plot(listAccuracyLiblinearScore, 'go', label=\"Liblinear\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Log Loss across all logistic regressions\")\n",
    "plt.xlabel(\"Regression #\")\n",
    "plt.ylabel(\"Log Loss Score\")\n",
    "plt.plot(listLogLossSagaScore, 'ko', label=\"Saga\")\n",
    "plt.plot(listAccuracyLiblinearScore, 'go', label=\"Liblinear\")\n",
    "libMean = [np.mean(listLogLossLiblinearScore)] * 100\n",
    "plt.plot(np.arange(0, 100), libMean, color=\"green\", label=\"Liblinear Mean\")\n",
    "sagaMean = [np.mean(listLogLossSagaScore)] * 100\n",
    "plt.plot(np.arange(0, 100), sagaMean, color=\"black\", label=\"Saga Mean\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Recall across all logistic regressions\")\n",
    "plt.xlabel(\"Regression #\")\n",
    "plt.ylabel(\"Recall Score\")\n",
    "plt.plot(listRecallSagaScore, 'ko', label=\"Saga\")\n",
    "plt.plot(listRecallLiblinearScore, 'go', label=\"Liblinear\")\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Precision across all logistic regressions\")\n",
    "plt.xlabel(\"Regression #\")\n",
    "plt.ylabel(\"Precision Score\")\n",
    "plt.plot(listPrecisionSagaScore, 'ko', label=\"Saga\")\n",
    "plt.plot(listPrecisionLiblinearScore, 'go', label=\"Liblinear\")\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"ROCAUC across all logistic regressions\")\n",
    "plt.xlabel(\"Regression #\")\n",
    "plt.ylabel(\"ROCAUC Score\")\n",
    "plt.plot(listROCAUCSagaScore, 'ko', label=\"Saga\")\n",
    "plt.plot(listROCAUCLiblinearScore, 'go', label=\"Liblinear\")\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listAccuracyDecTreeScore = []\n",
    "listRecallDecTreeScore = []\n",
    "listPrecisionDecTreeScore = []\n",
    "listROCAUCDecTreeScore = []\n",
    "\n",
    "#Runs the logistic regression 100 times and calculates the accuracy score\n",
    "for i in range (0,100):\n",
    "    #Randomly shuffles the x and y data while keeping matching order\n",
    "    temp = list(zip(x_array_transpose, y_array))\n",
    "    random.shuffle(temp)\n",
    "    x_shuffled, y_shuffled = zip(*temp)\n",
    "    x_shuffled = np.array(x_shuffled)\n",
    "    y_shuffled = np.array(y_shuffled)\n",
    "\n",
    "    #Splits data into train and test split\n",
    "    x_train = x_shuffled[0:80]\n",
    "    x_test = x_shuffled[80:99]\n",
    "    y_train = y_shuffled[0:80]\n",
    "    y_test = y_shuffled[80:99]\n",
    "\n",
    "    # Fitting Logistic Regression to the Training set\n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf.fit(x_train, y_train)\n",
    "\n",
    "    # Predicting the Test set results\n",
    "    y_pred = clf.predict(x_test)\n",
    "    y_pred_proba = clf.predict_proba(x_test)\n",
    "    listAccuracyDecTreeScore.append(metrics.accuracy_score(y_test, y_pred))\n",
    "    listRecallDecTreeScore.append(metrics.recall_score(y_test, y_pred))\n",
    "    listPrecisionDecTreeScore.append(metrics.precision_score(y_test, y_pred))\n",
    "    listROCAUCDecTreeScore.append(metrics.roc_auc_score(y_test, y_pred))\n",
    "\n",
    "print(\"Accuracy: \" + str(np.mean(listAccuracyDecTreeScore)))\n",
    "print(\"Recall: \" + str(np.mean(listRecallDecTreeScore)))\n",
    "print(\"Precision: \" + str(np.mean(listPrecisionDecTreeScore)))\n",
    "print(\"ROC-AUC: \" + str(np.mean(listROCAUCDecTreeScore)))\n",
    "\n",
    "tree.plot_tree(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Accuracy across all Saga/Decision Tree regressions\")\n",
    "plt.xlabel(\"Regression #\")\n",
    "plt.ylabel(\"Accuracy Score\")\n",
    "plt.plot(listAccuracyDecTreeScore, 'ro', label='Decision Tree')\n",
    "plt.plot(listAccuracySagaScore, 'ko', label='saga')\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Recall Score across all Saga/Decision Tree regressions\")\n",
    "plt.xlabel(\"Regression #\")\n",
    "plt.ylabel(\"Recall Score\")\n",
    "plt.plot(listRecallDecTreeScore, 'ro', label='Decision Tree')\n",
    "plt.plot(listRecallSagaScore, 'ko', label='saga')\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Precision across all Saga/Decision Tree regressions\")\n",
    "plt.xlabel(\"Regression #\")\n",
    "plt.ylabel(\"Precision Score\")\n",
    "plt.plot(listPrecisionDecTreeScore, 'ro', label='Decision Tree')\n",
    "plt.plot(listPrecisionSagaScore, 'ko', label='saga')\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"ROCAUC across all Saga/Decision Tree regressions\")\n",
    "plt.xlabel(\"Regression #\")\n",
    "plt.ylabel(\"ROCAUC Score\")\n",
    "plt.plot(listROCAUCDecTreeScore, 'ro', label='Decision Tree')\n",
    "plt.plot(listROCAUCSagaScore, 'ko', label='saga')\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Accuracy across all regressions\")\n",
    "plt.xlabel(\"Regression #\")\n",
    "plt.ylabel(\"Accuracy Score\")\n",
    "plt.plot(listAccuracyDecTreeScore, 'ro', label='Decision Tree')\n",
    "plt.plot(listAccuracySagaScore, 'ko', label='Saga')\n",
    "plt.plot(listAccuracyLiblinearScore, 'go', label=\"Liblinear\")\n",
    "libMean = [np.mean(listAccuracyLiblinearScore)] * 100\n",
    "plt.plot(np.arange(0, 100), libMean, color=\"green\", label=\"Liblinear Mean\")\n",
    "sagaMean = [np.mean(listAccuracySagaScore)] * 100\n",
    "plt.plot(np.arange(0, 100), sagaMean, color=\"black\", label=\"Saga Mean\")\n",
    "decTreeMean = [np.mean(listAccuracyDecTreeScore)] * 100\n",
    "plt.plot(np.arange(0, 100), decTreeMean, color=\"red\", label=\"Decision Tree Mean\")\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Recall Score across all regressions\")\n",
    "plt.xlabel(\"Regression #\")\n",
    "plt.ylabel(\"Recall Score\")\n",
    "plt.plot(listRecallDecTreeScore, 'ro', label='Decision Tree')\n",
    "plt.plot(listRecallSagaScore, 'ko', label='Saga')\n",
    "plt.plot(listRecallLiblinearScore, 'go', label=\"Liblinear\")\n",
    "libMean = [np.mean(listRecallLiblinearScore)] * 100\n",
    "plt.plot(np.arange(0, 100), libMean, color=\"green\", label=\"Liblinear Mean\")\n",
    "sagaMean = [np.mean(listRecallSagaScore)] * 100\n",
    "plt.plot(np.arange(0, 100), sagaMean, color=\"black\", label=\"Saga Mean\")\n",
    "decTreeMean = [np.mean(listRecallDecTreeScore)] * 100\n",
    "plt.plot(np.arange(0, 100), decTreeMean, color=\"red\", label=\"Decision Tree Mean\")\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Precision across all regressions\")\n",
    "plt.xlabel(\"Regression #\")\n",
    "plt.ylabel(\"Precision Score\")\n",
    "plt.plot(listPrecisionDecTreeScore, 'ro', label='Decision Tree')\n",
    "plt.plot(listPrecisionSagaScore, 'ko', label='Saga')\n",
    "plt.plot(listPrecisionLiblinearScore, 'go', label=\"Liblinear\")\n",
    "libMean = [np.mean(listPrecisionLiblinearScore)] * 100\n",
    "plt.plot(np.arange(0, 100), libMean, color=\"green\", label=\"Liblinear Mean\")\n",
    "sagaMean = [np.mean(listPrecisionSagaScore)] * 100\n",
    "plt.plot(np.arange(0, 100), sagaMean, color=\"black\", label=\"Saga Mean\")\n",
    "decTreeMean = [np.mean(listPrecisionDecTreeScore)] * 100\n",
    "plt.plot(np.arange(0, 100), decTreeMean, color=\"red\", label=\"Decision Tree Mean\")\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"ROCAUC across all regressions\")\n",
    "plt.xlabel(\"Regression #\")\n",
    "plt.ylabel(\"ROCAUC Score\")\n",
    "plt.plot(listROCAUCDecTreeScore, 'ro', label='Decision Tree')\n",
    "plt.plot(listROCAUCSagaScore, 'ko', label='Saga')\n",
    "plt.plot(listROCAUCLiblinearScore, 'go', label=\"Liblinear\")\n",
    "libMean = [np.mean(listROCAUCLiblinearScore)] * 100\n",
    "plt.plot(np.arange(0, 100), libMean, color=\"green\", label=\"Liblinear Mean\")\n",
    "sagaMean = [np.mean(listROCAUCSagaScore)] * 100\n",
    "plt.plot(np.arange(0, 100), sagaMean, color=\"black\", label=\"Saga Mean\")\n",
    "decTreeMean = [np.mean(listROCAUCDecTreeScore)] * 100\n",
    "plt.plot(np.arange(0, 100), decTreeMean, color=\"red\", label=\"Decision Tree Mean\")\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
